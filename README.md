# ETL-Using-AWS-Services

# AWS Data Pipeline â€“ Stocks & Weather

End-to-end **AWS Data Engineering Pipeline** that ingests data from **Kafka (via Docker + Python)** into **S3 (Raw)**, transforms it using **AWS Glue (PySpark)**, stores curated Parquet datasets in **S3 (Curated)**, and exposes it for querying via **Athena** and visualization in **Power BI**.  

---

## ğŸ—ï¸ Architecture

<img width="1748" height="1240" alt="Image" src="https://github.com/user-attachments/assets/692b682f-d83d-451c-ae8e-9ff28040e159" />

**Flow:**
1. **Kafka (Docker + Python)** â†’ Streams **stocks** and **weather** data into **S3 Raw**.
2. **AWS Glue Crawlers** â†’ Crawl `raw-data-24-08` bucket and create schemas in Glue Data Catalog.
3. **AWS Glue ETL Jobs** â†’  
   - `stocks_transform.py` â†’ writes curated parquet to `s3://curated-data-24-08/ready_stocks/`  
   - `weather_transform.py` â†’ writes curated parquet to `s3://curated-data-24-08/ready_weather/`
4. **Athena** â†’ Query raw and curated datasets directly.
5. **Power BI** â†’ Uses Athena connector for dashboards & reporting.

---

## ğŸ“‚ Folder Structure
```
aws-data-pipeline-repo/
â”‚â”€â”€ fetching-data-script/        # Kafka â†’ S3 Raw ingestion scripts
â”‚   â”œâ”€â”€ stocks_producer.py
â”‚   â”œâ”€â”€ weather_producer.py
â”‚   â””â”€â”€ .env                     # Kafka & AWS configs (never commit secrets!)
â”‚
â”‚â”€â”€ glue-etl-job-script/         # AWS Glue ETL jobs (PySpark)
â”‚   â”œâ”€â”€ stocks_transform.py
â”‚   â””â”€â”€ weather_transform.py

```


